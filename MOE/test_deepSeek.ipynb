{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional, Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- block_size:\n",
    "\n",
    "    * The block_size refers to the size of the input sequence chunks or blocks that the model processes. In transformer models, this is often related to the maximum sequence length or the size of attention windows.\n",
    "\n",
    "    * In the context of MoE, block_size might determine how inputs are split or grouped for routing to experts.\n",
    "\n",
    "- rank and world_size:\n",
    "\n",
    "These terms are related to distributed training:\n",
    "\n",
    "    * world_size: The total number of processes or devices participating in distributed training. For example, if you are using 4 GPUs, world_size would be 4.\n",
    "        * world_size is used to divide the total number of experts (n_routed_experts) among the available devices.\n",
    "\n",
    "    * rank: The unique identifier for each process or device in the distributed system. For example, in a 4-GPU setup, rank would be 0, 1, 2, or 3\n",
    "        *rank determines which subset of experts is handled by the current device. For example, if world_size = 4 and rank = 0, the device handles the first quarter of the experts. \n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Input Routing:\n",
    "\n",
    "    - The input tensor is passed through the Gate module, which computes routing scores and selects the top-k experts for each input.\n",
    "\n",
    "2. Expert Computation:\n",
    "\n",
    "    - The input is routed to the selected experts, and each expert processes its assigned inputs independently.\n",
    "\n",
    "3. Shared Experts:\n",
    "\n",
    "    - In addition to the routed experts, shared experts are applied to all inputs. This ensures that common features are captured across all inputs.\n",
    "\n",
    "4. Output Aggregation:\n",
    "\n",
    "    - The outputs from the routed experts and shared experts are combined to produce the final output.\n",
    "\n",
    "5. Distributed Training:\n",
    "\n",
    "    - If world_size > 1, the outputs from different devices are aggregated using dist.all_reduce to synchronize the results across devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 1\n",
    "rank = 0\n",
    "block_size = 128\n",
    "gemm_impl: Literal[\"bf16\", \"fp8\"] = \"bf16\"\n",
    "attn_impl: Literal[\"naive\", \"absorb\"] = \"absorb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mixture of Experts (MoE) architecture is a neural network design that leverages multiple specialized sub-networks \n",
    "\n",
    "(called \"experts\") to process different parts of the input data. The key idea is to route different inputs to different experts,\n",
    "\n",
    " allowing the model to scale efficiently while maintaining high performance. Below is a detailed explanation of the DeepSeek MoE implementation provided in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    \"\"\"\n",
    "    Data class for defining model arguments and hyperparameters.\n",
    "\n",
    "    Attributes:\n",
    "        max_batch_size (int): Maximum batch size.\n",
    "        max_seq_len (int): Maximum sequence length.\n",
    "        dtype (Literal[\"bf16\", \"fp8\"]): Data type for computations.\n",
    "        vocab_size (int): Vocabulary size.\n",
    "        dim (int): Model dimension.\n",
    "        inter_dim (int): Intermediate dimension for MLP layers.\n",
    "        moe_inter_dim (int): Intermediate dimension for MoE layers.\n",
    "        n_layers (int): Number of transformer layers.\n",
    "        n_dense_layers (int): Number of dense layers in the model.\n",
    "        n_heads (int): Number of attention heads.\n",
    "        n_routed_experts (int): Number of routed experts for MoE layers.\n",
    "        n_shared_experts (int): Number of shared experts for MoE layers.\n",
    "        n_activated_experts (int): Number of activated experts in MoE layers.\n",
    "        n_expert_groups (int): Number of expert groups.\n",
    "        n_limited_groups (int): Number of limited groups for MoE routing.\n",
    "        score_func (Literal[\"softmax\", \"sigmoid\"]): Scoring function for MoE routing.\n",
    "        route_scale (float): Scaling factor for routing scores.\n",
    "        q_lora_rank (int): LoRA rank for query projections.\n",
    "        kv_lora_rank (int): LoRA rank for key-value projections.\n",
    "        qk_nope_head_dim (int): Dimension for query-key projections without positional embeddings.\n",
    "        qk_rope_head_dim (int): Dimension for query-key projections with rotary embeddings.\n",
    "        v_head_dim (int): Dimension for value projections.\n",
    "        original_seq_len (int): Original sequence length.\n",
    "        rope_theta (float): Base for rotary positional encoding.\n",
    "        rope_factor (float): Scaling factor for extended sequence lengths.\n",
    "        beta_fast (int): Fast beta correction factor.\n",
    "        beta_slow (int): Slow beta correction factor.\n",
    "        mscale (float): Scaling factor for extended attention.\n",
    "    \"\"\"\n",
    "    max_batch_size: int = 8\n",
    "    max_seq_len: int = 4096 * 4\n",
    "    dtype: Literal[\"bf16\", \"fp8\"] = \"bf16\"\n",
    "    vocab_size: int = 102400\n",
    "    dim: int = 2048\n",
    "    inter_dim: int = 10944\n",
    "    moe_inter_dim: int = 1408\n",
    "    n_layers: int = 27\n",
    "    n_dense_layers: int = 1\n",
    "    n_heads: int = 16\n",
    "    # moe\n",
    "    n_routed_experts: int = 64\n",
    "    n_shared_experts: int = 2\n",
    "    n_activated_experts: int = 6\n",
    "    n_expert_groups: int = 1\n",
    "    n_limited_groups: int = 1\n",
    "    score_func: Literal[\"softmax\", \"sigmoid\"] = \"softmax\"\n",
    "    route_scale: float = 1.\n",
    "    # mla\n",
    "    q_lora_rank: int = 0\n",
    "    kv_lora_rank: int = 512\n",
    "    qk_nope_head_dim: int = 128\n",
    "    qk_rope_head_dim: int = 64\n",
    "    v_head_dim: int = 128\n",
    "    # yarn\n",
    "    original_seq_len: int = 4096\n",
    "    rope_theta: float = 10000.0\n",
    "    rope_factor: float = 40\n",
    "    beta_fast: int = 32\n",
    "    beta_slow: int = 1\n",
    "    mscale: float = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gate:\n",
    "\n",
    "The Gate module is responsible for routing inputs to the appropriate experts. It computes scores for each expert and selects the top-k experts for each input.\n",
    "\n",
    "Key attributes:\n",
    "\n",
    "- dim: Input feature dimensionality.\n",
    "\n",
    "- topk: Number of experts activated for each input.\n",
    "\n",
    "- n_groups: Number of groups for routing.\n",
    "\n",
    "- score_func: Scoring function (softmax or sigmoid) to compute routing weights.\n",
    "\n",
    "- route_scale: Scaling factor for routing weights.\n",
    "\n",
    "The forward method computes routing weights and indices for the selected experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.  self.topk = args.n_activated_experts**\n",
    " - Meaning:\n",
    "\n",
    "    * self.topk specifies the number of experts that will be activated for each input. In other words, for every input, the gating mechanism selects the top-k experts (where k = n_activated_experts) to process that input.\n",
    "\n",
    " - Purpose:\n",
    "\n",
    "   * Instead of sending every input to all experts (which would be computationally expensive), the MoE model only activates a small subset of experts for each input. This makes the model more efficient while still allowing it to leverage specialized experts.\n",
    "\n",
    " -  Example:\n",
    "\n",
    "   * If n_activated_experts = 6, then for each input, the gating mechanism will select the 6 most relevant experts (based on the routing scores) to process that input.\n",
    "\n",
    "**2.  self.n_groups = args.n_expert_groups**\n",
    " - Meaning:\n",
    "\n",
    "   * self.n_groups defines the number of expert groups used in the routing mechanism. Experts can be divided into groups, and the gating mechanism can route inputs to specific groups of experts.\n",
    "\n",
    " - Purpose:\n",
    "\n",
    "   * Grouping experts allows for more structured and efficient routing. For example, experts in the same group might specialize in similar types of inputs, and the gating mechanism can route inputs to the most relevant group(s).\n",
    "\n",
    " - Example:\n",
    "\n",
    "   * If n_expert_groups = 4, the experts are divided into 4 groups. The gating mechanism will first decide which group(s) to route the input to, and then select the top-k experts within those groups.\n",
    "\n",
    "**3.  self.topk_groups = args.n_limited_groups**\n",
    " - Meaning:\n",
    "\n",
    "   * self.topk_groups specifies the number of expert groups that will be activated for each input. In other words, for each input, the gating mechanism will select the top-k groups (where k = n_limited_groups) to route the input to.\n",
    "\n",
    " - Purpose:\n",
    "\n",
    "   * This parameter further refines the routing process by limiting the number of groups that can be activated for each input. This helps reduce computational overhead and ensures that inputs are routed to the most relevant groups of experts.\n",
    "\n",
    " - Example:\n",
    "\n",
    "   * If n_limited_groups = 2, then for each input, the gating mechanism will select the 2 most relevant groups of experts and route the input to those groups. Within those groups, the top-k experts (as defined by n_activated_experts) will be activated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How These Parameters Work Together\n",
    "**1. Routing Process:**\n",
    "\n",
    "- For each input, the gating mechanism computes routing scores for all experts.\n",
    "\n",
    "- If n_groups > 1, the experts are divided into groups, and the routing scores are computed for each group.\n",
    "\n",
    "* The gating mechanism selects the top-k groups (topk_groups) and the top-k experts (topk) within those groups.\n",
    "\n",
    "**2. Example Workflow:**\n",
    "\n",
    "- Suppose:\n",
    "\n",
    "    * n_expert_groups = 4 (experts are divided into 4 groups).\n",
    "\n",
    "    * n_limited_groups = 2 (for each input, the top 2 groups are selected).\n",
    "\n",
    "    * n_activated_experts = 6 (for each input, the top 6 experts are activated).\n",
    "\n",
    "- For a given input:\n",
    "\n",
    "    * The gating mechanism computes routing scores for all 4 groups.\n",
    "\n",
    "    * It selects the top 2 groups with the highest scores.\n",
    "\n",
    "    * Within those 2 groups, it selects the top 6 experts (in total) with the highest scores.\n",
    "\n",
    "    * The input is routed to those 6 experts for processing.\n",
    "\n",
    "#### Key Relationship\n",
    "* n_groups defines how the experts are organized into groups before routing.\n",
    "\n",
    "* topk_groups determines how many of these groups will be activated for each input.\n",
    "\n",
    "* topk determines how many experts will be activated within the selected groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    \"\"\"\n",
    "    Gating mechanism for routing inputs in a mixture-of-experts (MoE) model.\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): Dimensionality of input features.\n",
    "        topk (int): Number of top experts activated for each input.\n",
    "        n_groups (int): Number of groups for routing.\n",
    "        topk_groups (int): Number of groups to route inputs to.\n",
    "        score_func (str): Scoring function ('softmax' or 'sigmoid').\n",
    "        route_scale (float): Scaling factor for routing weights.\n",
    "        weight (torch.nn.Parameter): Learnable weights for the gate.\n",
    "        bias (Optional[torch.nn.Parameter]): Optional bias term for the gate.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the Gate module.\n",
    "\n",
    "        Args:\n",
    "            args (ModelArgs): Model arguments containing gating parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = args.dim\n",
    "        self.topk = args.n_activated_experts\n",
    "        self.n_groups = args.n_expert_groups\n",
    "        self.topk_groups = args.n_limited_groups\n",
    "        self.score_func = args.score_func\n",
    "        self.route_scale = args.route_scale\n",
    "        self.weight = nn.Parameter(torch.empty(args.n_routed_experts, args.dim))\n",
    "        self.bias = nn.Parameter(torch.empty(args.n_routed_experts)) if self.dim == 7168 else None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for the gating mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Routing weights and selected expert indices.\n",
    "        \"\"\"\n",
    "         # (scores)  =  u_t * e_i -> affine between token and experts\n",
    "        scores = F.linear(x, self.weight)\n",
    "\n",
    "        #s_i,t = softmax(scores) or s_i,t =sigmoid(scores)\n",
    "        if self.score_func == \"softmax\":\n",
    "            scores = scores.softmax(dim=-1, dtype=torch.float32)\n",
    "        else:\n",
    "            scores = scores.sigmoid()\n",
    "        original_scores = scores\n",
    "\n",
    "        # Auxiliary-Loss-Free Load Balancing\n",
    "        if self.bias is not None:\n",
    "            scores = scores + self.bias\n",
    "        # equation 14 paper deepseek v3\n",
    "        if self.n_groups > 1:\n",
    "            #  Reshape the affinity scores into groups.\n",
    "            scores = scores.view(x.size(0), self.n_groups, -1)  #\n",
    "            # 1. Grouping:\n",
    "            if self.bias is None:\n",
    "                # get the maximum values  along the last dimension \n",
    "                group_scores = scores.amax(dim=-1)  \n",
    "            else:\n",
    "                group_scores = scores.topk(2, dim=-1)[0].sum(dim=-1)\n",
    "            # 2. Select Top-k Groups\n",
    "            indices = group_scores.topk(self.topk_groups, dim=-1)[1]\n",
    "            # 3.  Masking: The scores of unselected groups are set to −∞ (effectively 0 in the softmax or sigmoid function).\n",
    "            mask = scores.new_ones(x.size(0), self.n_groups, dtype=bool).scatter_(1, indices, False)\n",
    "            # 4. Flattening: The scores are flattened back to their original shape, with only the top K_r groups retained.\n",
    "            scores = scores.masked_fill_(mask.unsqueeze(-1), float(\"-inf\")).flatten(1)\n",
    "\n",
    "        indices = torch.topk(scores, self.topk, dim=-1)[1] # \n",
    "\n",
    "        weights = original_scores.gather(1, indices)\n",
    "\n",
    "        # Normalize if fucntion is sigmoid. sincre softmax is already normalized \n",
    "        if self.score_func == \"sigmoid\":\n",
    "            weights /= weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # last part of equation  12 DeepSek V3 paper \n",
    "        weights *= self.route_scale\n",
    "        \n",
    "        return weights.type_as(x), indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expert:\n",
    "\n",
    "- The Expert module represents a single expert in the MoE model. It consists of linear layers (w1, w2, w3) that transform the input data.\n",
    "\n",
    "- The forward method applies the expert's computation to the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    Expert layer for Mixture-of-Experts (MoE) models.\n",
    "\n",
    "    Attributes:\n",
    "        w1 (nn.Module): Linear layer for input-to-hidden transformation.\n",
    "        w2 (nn.Module): Linear layer for hidden-to-output transformation.\n",
    "        w3 (nn.Module): Additional linear layer for feature transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, inter_dim: int):\n",
    "        \"\"\"\n",
    "        Initializes the Expert layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Input and output dimensionality.\n",
    "            inter_dim (int): Hidden layer dimensionality.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, inter_dim)\n",
    "        self.w2 = nn.Linear(inter_dim, dim)\n",
    "        self.w3 = nn.Linear(dim, inter_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Expert layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after expert computation.\n",
    "        \"\"\"\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (MLP) used as a feed-forward layer.\n",
    "\n",
    "    Attributes:\n",
    "        w1 (nn.Module): Linear layer for input-to-hidden transformation.\n",
    "        w2 (nn.Module): Linear layer for hidden-to-output transformation.\n",
    "        w3 (nn.Module): Additional linear layer for feature transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, inter_dim: int):\n",
    "        \"\"\"\n",
    "        Initializes the MLP layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Input and output dimensionality.\n",
    "            inter_dim (int): Hidden layer dimensionality.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, inter_dim)\n",
    "        self.w2 = nn.Linear(inter_dim, dim)\n",
    "        self.w3 = nn.Linear(dim, inter_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the MLP layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after MLP computation.\n",
    "        \"\"\"\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoE (Mixture of Experts):\n",
    "\n",
    "- The MoE module combines multiple experts and a gating mechanism to route inputs to the appropriate experts.\n",
    "\n",
    "- Key attributes:\n",
    "\n",
    "    * n_routed_experts: Total number of experts.\n",
    "\n",
    "    * n_local_experts: Number of experts handled locally in distributed systems.\n",
    "\n",
    "    * n_activated_experts: Number of experts activated for each input.\n",
    "\n",
    "    * gate: Gating mechanism for routing.\n",
    "\n",
    "    * experts: List of expert modules.\n",
    "\n",
    "    * shared_experts: Shared experts applied to all inputs.\n",
    "\n",
    "The forward method routes inputs to experts, computes their outputs, and combines them with shared expert outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture-of-Experts (MoE) module.\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): Dimensionality of input features.\n",
    "        n_routed_experts (int): Total number of experts in the model.\n",
    "        n_local_experts (int): Number of experts handled locally in distributed systems.\n",
    "        n_activated_experts (int): Number of experts activated for each input.\n",
    "        gate (nn.Module): Gating mechanism to route inputs to experts.\n",
    "        experts (nn.ModuleList): List of expert modules.\n",
    "        shared_experts (nn.Module): Shared experts applied to all inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the MoE module.\n",
    "\n",
    "        Args:\n",
    "            args (ModelArgs): Model arguments containing MoE parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = args.dim\n",
    "        assert args.n_routed_experts % world_size == 0, f\"Number of experts must be divisible by world size (world_size={world_size})\"\n",
    "        self.n_routed_experts = args.n_routed_experts\n",
    "        self.n_local_experts = args.n_routed_experts // world_size\n",
    "        self.n_activated_experts = args.n_activated_experts\n",
    "        self.experts_start_idx = rank * self.n_local_experts\n",
    "        self.experts_end_idx = self.experts_start_idx + self.n_local_experts\n",
    "        self.gate = Gate(args)\n",
    "        self.experts = nn.ModuleList([Expert(args.dim, args.moe_inter_dim) if self.experts_start_idx <= i < self.experts_end_idx else None for i in range(self.n_routed_experts)])\n",
    "        self.shared_experts = MLP(args.dim, args.n_shared_experts * args.moe_inter_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the MoE module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after expert routing and computation.\n",
    "        \"\"\"\n",
    "        shape = x.size()\n",
    "        x = x.view(-1, self.dim)\n",
    "        weights, indices = self.gate(x)\n",
    "        y = torch.zeros_like(x)\n",
    "        counts = torch.bincount(indices.flatten(), minlength=self.n_routed_experts).tolist()\n",
    "        for i in range(self.experts_start_idx, self.experts_end_idx):\n",
    "            if counts[i] == 0:\n",
    "                continue\n",
    "            expert = self.experts[i]\n",
    "            idx, top = torch.where(indices == i)\n",
    "            y[idx] += expert(x[idx]) * weights[idx, top, None]\n",
    "        z = self.shared_experts(x)\n",
    "\n",
    "        return (y + z).view(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
