{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet matplotlib\n",
    "# !pip install --quiet seaborn\n",
    "# !pip install --quiet tqdm\n",
    "# ! pip install --quiet ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial15/Vision_Transformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "## Imports for plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf')  # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## To run JAX on TPU in Google Colab, uncomment the two lines below\n",
    "# import jax.tools.colab_tpu\n",
    "# jax.tools.colab_tpu.setup_tpu()\n",
    "\n",
    "## JAX\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "# Seeding for random operations\n",
    "main_rng = random.PRNGKey(42)\n",
    "\n",
    "## Flax (NN in JAX)\n",
    "try:\n",
    "    import flax\n",
    "except ModuleNotFoundError: # Install flax if missing\n",
    "    !pip install --quiet flax\n",
    "    import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "\n",
    "## Optax (Optimizers in JAX)\n",
    "try:\n",
    "    import optax\n",
    "except ModuleNotFoundError: # Install optax if missing\n",
    "    !pip install --quiet optax\n",
    "    import optax\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial6_jax\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: TFRT_CPU_0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Device:\", jax.devices()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    attn_logits = jnp.matmul(q, jnp.swapaxes(k, -2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = jnp.where(mask == 0, -9e15, attn_logits)\n",
    "    attention = nn.softmax(attn_logits, axis=-1)\n",
    "    values = jnp.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.60576403  0.7990441 ]\n",
      " [-0.908927   -0.63525754]\n",
      " [-1.2226585  -0.83226097]]\n",
      "K\n",
      " [[-0.47417238 -1.2504351 ]\n",
      " [-0.17678244 -0.04917514]\n",
      " [-0.41177532 -0.39363015]]\n",
      "V\n",
      " [[ 1.3116323   0.21555556]\n",
      " [ 0.41164538 -0.28955024]\n",
      " [-0.96516913  0.4492738 ]]\n",
      "Values\n",
      " [[0.12734914 0.06441191]\n",
      " [0.4115729  0.17320421]\n",
      " [0.46902645 0.1854193 ]]\n",
      "Attention\n",
      " [[0.20383833 0.4564296  0.33973208]\n",
      " [0.46830934 0.2255167  0.30617398]\n",
      " [0.51187545 0.19520193 0.29292265]]\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "main_rng, rand1 = random.split(main_rng)\n",
    "qkv = random.normal(rand1, (3, L, d_k))\n",
    "q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Understand Multi-head Latent Attention (MLA)\n",
    "\n",
    "MLA modifies the standard Multi-head Attention (MHA) by:\n",
    "\n",
    " 1. **Compressing Keys and Values:** Instead of storing full KV matrices, MLA projects them into a low-dimensional latent space (e.g., from $d_{model}$ to $latent_dim$, where $latent_dim << d_{model}$) using a down-projection matrix.\n",
    " 2. **Reconstructing Keys and Values:** During attention computation, latent vectors are up-projected back to the original dimension using separate matrices for keys and values.\n",
    " 3. **Decoupled RoPE:** Positional information (via RoPE) is handled separately to maintain compatibility with KV compression.\n",
    " 4. **Query Handling:** Queries can also be compressed into a latent space and then up-projected, though this is optional depending on the design.\n",
    "\n",
    " The goal is to reduce the KV cache size (e.g., from $O(num_heads * d_h)$ to $O(latent_dim)$ per token, where $num_heads$ is the **number of heads** and $d_h$ is **the head dimension**) while preserving attention quality.\n",
    "\n",
    " $X\\in \\mathbb{R}^{T x D }$, where $T$ represents the sequence length and $D$ is the hidden dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Define the Architecture\n",
    "\n",
    "- Down-projection: A linear layer to compress KV into a latent vector.\n",
    "- Up-projection: Two linear layers to reconstruct keys and values from the latent vector.\n",
    "- Query projection: Optionally compress queries into a latent space, though typically queries remain in full dimension for flexibility.\n",
    "- RoPE: A custom implementation for positional embeddings, decoupled from KV compression.\n",
    "- Attention computation: Standard scaled dot-product attention using the reconstructed keys and values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rotary Positional Embedding\n",
    "def rotary_embedding(x, max_seq_len, dim):\n",
    "    \"\"\"Apply RoPE to input tensor x.\"\"\"\n",
    "    positions = jnp.arange(max_seq_len, dtype=jnp.float32)\n",
    "    freqs = 1.0 / (10000 ** (jnp.arange(0, dim, 2, dtype=jnp.float32) / dim))\n",
    "    angles = positions[:, None] * freqs[None, :]\n",
    "    sin, cos = jnp.sin(angles),jnp.cos(angles)\n",
    "    \"\"\"Apply rotary embeddings to input tensor x.\"\"\"\n",
    "    x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "    x_rotated = jnp.concatenate([x1 * cos - x2 * sin, x1 * sin + x2 * cos], axis=-1)\n",
    "    return x_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scaled_dot_product(q, k, v, mask=None):\n",
    "#     d_k = q.shape[-1]\n",
    "#     attn_logits = jnp.matmul(q, jnp.swapaxes(k, -2, -1))\n",
    "#     attn_logits = attn_logits / math.sqrt(d_k)\n",
    "#     if mask is not None:\n",
    "#         attn_logits = jnp.where(mask == 0, -9e15, attn_logits)\n",
    "#     attention = nn.softmax(attn_logits, axis=-1)\n",
    "#     values = jnp.matmul(attention, v)\n",
    "#     return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    denominator = q.shape[-1]  # d_h + d_h_r\n",
    "    scores = jnp.matmul(q, k.transpose(0, 1, 3, 2)) / jnp.sqrt(denominator)  # Normalize by sqrt(d_h + d_h^R) transpose[B, n_h, d_h + d_h_r, L]\n",
    "    if mask is not None:\n",
    "        scores = scores + mask\n",
    "    attention = nn.softmax(scores, axis=-1)\n",
    "    values = jnp.matmul(attention, v)\n",
    "    return values, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to support different mask shapes.\n",
    "# Output shape supports (B, number of heads, seq length, seq length)\n",
    "# If 2D: broadcasted over batch size and number of heads\n",
    "# If 3D: broadcasted over number of heads\n",
    "# If 4D: leave as is\n",
    "def expand_mask(mask):\n",
    "    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with L x L\"\n",
    "    if mask.ndim == 3:\n",
    "        mask = mask.unsqueeze(1)\n",
    "    while mask.ndim < 4:\n",
    "        mask = mask.unsqueeze(0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original MHA with ROPE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head Latent Attention Module\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    d_model: int  # Output dimension (d_model)\n",
    "    n_h: int  # Number of parallel heads (h)\n",
    "    d_c : int # Latent dimension for compression (d_c) kv\n",
    "    d_c_ : int # Latent dimension for compression (d_c') queries\n",
    "    d_h_R: int  # Rotated dimension for RoPE (d_h^R)\n",
    "\n",
    "    def setup(self):\n",
    "        self.d_h = self.d_model // self.n_h\n",
    "\n",
    "        # Down-projection for KV\n",
    "        self.dkv_proj = nn.Dense(\n",
    "            features=self.d_c, \n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )  # W^DKV\n",
    "        \n",
    "        # Up-projection for keys (compressed part)\n",
    "        self.uk_proj = nn.Dense(\n",
    "            features=self.d_model,   # d_h * n_h = d_model\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )  # W^UK\n",
    "\n",
    "        # Up-projection for values\n",
    "        self.uv_proj = nn.Dense(\n",
    "            features=self.d_model, # d_h * n_h = d_model\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )  # W^UV\n",
    "\n",
    "         # Down-projection for queries\n",
    "        self.dq_proj = nn.Dense(\n",
    "            features=self.d_c_,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )  # W^DQ\n",
    "\n",
    "\n",
    "        # Up-projection for queries (compressed part)\n",
    "        self.uq_proj = nn.Dense(\n",
    "            features=self.d_model,  # W^{UQ} (d_h n_h x d_c' (d_c_))  c^Q (d_c')\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )  # W^UQ\n",
    "\n",
    "\n",
    "        # Rotated projection for queries and keys (shared)\n",
    "        self.qr_proj  = nn.Dense(\n",
    "            features=self.d_h_R * self.n_h, # d_h^R * n_h\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )  # W^QR \n",
    "\n",
    "        # Rotated projection for keys (shared)\n",
    "        self.kr_proj = nn.Dense(\n",
    "            features=self.d_h_R, # d_h^R * d x d = d_h^R\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )  # W^KR\n",
    "\n",
    "        # Output projection\n",
    "        self.o_proj = nn.Dense(\n",
    "            features=self.d_model,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        B, L, _ = x.shape  # Batch, Sequence Length, Dimension\n",
    "    \n",
    "\n",
    "        if mask is not None:\n",
    "            mask = expand_mask(mask)\n",
    "\n",
    "      \n",
    "        # Down-project KV\n",
    "        c_kv = self.dkv_proj(x)  # c_t^KV = W^DKV h_t [B, L, latent_dim (d_c)]\n",
    "        # Up-project keys (compressed part)\n",
    "        k_c = self.uk_proj(c_kv)  # k_t^C = W^UK c_t^KV [B, L, dim]\n",
    "        # Up-project values\n",
    "        v_c = self.uv_proj(c_kv)  # v_t^C = W^UV c_t^KV [B, L, dim]\n",
    "   \n",
    "        # Down-project queries\n",
    "        c_q = self.dq_proj(x)  # c_t^Q = W^DQ h_t [B, L, latent_dim(d_c')]\n",
    "        # Up-project queries (compressed part)\n",
    "        q_c = self.uq_proj(c_q)  # q_t^C = W^UQ c_t^Q [B, L, dim]\n",
    "\n",
    "\n",
    "        # Rotated part for queries\n",
    "        q_r = self.qr_proj(c_q)  # W^QR c_t^Q [B, L, d_h^R * n_h]\n",
    "        q_r = rotary_embedding(q_r, L, self.d_h_R * self.n_h)  # Apply RoPE  [B, L, d_h^R * n_h]\n",
    "        q_r = q_r.reshape(B, L, self.n_h, self.d_h_R)  # Reshape to [B, L, n_h, d_h^R]\n",
    "        q_r = q_r.transpose(0, 2, 1, 3)  # [B, n_h, L, d_h^R]\n",
    "\n",
    "        q_c = q_c.reshape(B, L, self.n_h, self.d_h)\n",
    "        q_c = q_c.transpose(0,2,1,3) # [B, n_h, L, d_h]\n",
    "     \n",
    "        q = jnp.concatenate([q_c, q_r], axis=-1)  # [B, n_h, L, d_h + d_h^R]\n",
    "        \n",
    "        # Rotated part for keys (shared)\n",
    "        k_r = self.kr_proj(x) # k_t^R = W^KR h_t [B, L, d_h^R]\n",
    "        k_r = rotary_embedding(k_r, L, self.d_h_R)  # Apply RoPE  [B, L, d_h^R]\n",
    "        k_r = k_r.reshape(B, L, 1, self.d_h_R)# [B, L, n_h = 1, d_h^R]\n",
    "        k_r = jnp.repeat(k_r, self.n_h, axis=2)  # Repeat along head dimension to [B, L, n_h, d_h^R]\n",
    "        k_r = k_r.transpose(0, 2, 1, 3) # [B, n_h,  L, d_h^R]\n",
    "     \n",
    "        k_c = k_c.reshape(B, L, self.n_h, self.d_model // self.n_h) # n_h x d_h (d // n_h) [B, L, n_h, d_h]\n",
    "        k_c  = k_c.transpose(0, 2, 1, 3)\n",
    "        k = jnp.concatenate([k_c, k_r], axis=-1)  # [B, n_h, L, d_h^R + d_h]\n",
    "  \n",
    "        \n",
    "        v_c = v_c.reshape(B, L, self.n_h, self.d_model // self.n_h) #[B, L, n_h, d_h]\n",
    "        v = v_c.transpose(0, 2, 1, 3)  # [B, n_h, L, d_h]\n",
    "\n",
    "        # Attention computation\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask) #[B, n_h, L , d_h]\n",
    "       \n",
    "        # Reshape and project output\n",
    "        values = values.transpose(0, 2, 1, 3)  #  [B, L, n_h, d_h]\n",
    "        values = values.reshape(B, L, self.n_h * self.d_model // self.n_h) #[B, L, n_h * d_h]\n",
    "        o = self.o_proj(values)  # [B, L, d_model]\n",
    "\n",
    "        return o, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    dim = 512  # Output dimension\n",
    "    num_heads = 8\n",
    "    latent_dim = 128  # Latent dimension (d_c)\n",
    "    rotary_dim = 32  # Rotated dimension (d_h^R)\n",
    "    seq_len = 64\n",
    "    batch_size = 2\n",
    "    \n",
    "    # d_model: int  # Output dimension (d_model)\n",
    "    # n_h: int  # Number of parallel heads (h)\n",
    "    # d_c : int # Latent dimension for compression (d_c) kv\n",
    "    # d_c_ : int # Latent dimension for compression (d_c') queries\n",
    "    # d_h_R: int  # Rotated dimension for RoPE (d_h^R)\n",
    "    # Initialize model\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    model = MultiHeadLatentAttention(\n",
    "        d_model=dim, n_h=num_heads, d_c=latent_dim,d_c_ = latent_dim, d_h_R=rotary_dim\n",
    "    )\n",
    "    x = jax.random.normal(rng, (batch_size, seq_len, dim))\n",
    "    params = model.init(rng, x)\n",
    "\n",
    "    # Forward pass\n",
    "    output, attention = model.apply(params, x)\n",
    "    print(\"Output shape:\", output.shape)  # Should be [batch_size, seq_len, dim]\n",
    "    print(\"Attention shape:\", attention.shape)  # Should be [batch_size, num_heads, seq_len, seq_len]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "64.2 ms ± 4.73 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scaled Dot-Product Attention\n",
    "# def scaled_dot_product(q, k, v, mask=None):\n",
    "#     denominator = q.shape[-1]  # d_h \n",
    "#     scores = jnp.matmul(q, k.transpose(0, 1, 3, 2)) / jnp.sqrt(denominator)  # Normalize by sqrt(d_h + d_h^R) transpose[B, n_h, d_h + d_h_r, L]\n",
    "#     if mask is not None:\n",
    "#         scores = scores + mask\n",
    "#     attention = nn.softmax(scores, axis=-1)\n",
    "#     values = jnp.matmul(attention, v)\n",
    "#     return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head Latent Attention Module\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    d_model: int  # Output dimension (d_model)\n",
    "    n_h: int  # Number of parallel heads (h)\n",
    "    d_c : int # Latent dimension for compression (d_c) kv\n",
    "\n",
    "    def setup(self):\n",
    "        self.d_h = self.d_model // self.n_h\n",
    "\n",
    "        # Down-projection for KV\n",
    "        self.dkv_proj = nn.Dense(\n",
    "            features=self.d_c, \n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )  # W^DKV\n",
    "        \n",
    "        # Up-projection for keys (compressed part)\n",
    "        self.uk_proj = nn.Dense(\n",
    "            features=self.d_model,   # d_h * n_h = d_model\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )  # W^UK\n",
    "\n",
    "        # Up-projection for values\n",
    "        self.uv_proj = nn.Dense(\n",
    "            features=self.d_model, # d_h * n_h = d_model\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )  # W^UV\n",
    "\n",
    "        # projection for queries (compressed part)\n",
    "        self.q_proj = nn.Dense(\n",
    "            features=self.d_model,  # W^{UQ} (d_h n_h x d_c' (d_c_))  c^Q (d_c')\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )  # W^Q\n",
    "\n",
    "        # Output projection\n",
    "        self.o_proj = nn.Dense(\n",
    "            features=self.d_model,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        B, L, _ = x.shape  # Batch, Sequence Length, Dimension\n",
    "    \n",
    "\n",
    "        if mask is not None:\n",
    "            mask = expand_mask(mask)\n",
    "\n",
    "      \n",
    "        # Down-project KV\n",
    "        c_kv = self.dkv_proj(x)  # c_t^KV = W^DKV h_t [B, L, d_c]\n",
    "        # Up-project keys (compressed part)\n",
    "        k = self.uk_proj(c_kv)  # k_t^C = W^UK c_t^KV [B, L, dim]\n",
    "        k = k.reshape(B, L, self.n_h, self.d_h)\n",
    "        k = k.transpose(0, 2, 1, 3)\n",
    "        # Up-project values\n",
    "        v = self.uv_proj(c_kv)  # v_t^C = W^UV c_t^KV [B, L, dim]\n",
    "        v = v.reshape(B, L, self.n_h, self.d_h)\n",
    "        v = v.transpose(0, 2, 1, 3)\n",
    "  \n",
    "        # project queries\n",
    "        q = self.q_proj(x)  # c_t^Q = W^Q h_t [B, L, dim]\n",
    "        q = q.reshape(B, L, self.n_h, self.d_h)\n",
    "        q = q.transpose(0, 2, 1, 3)\n",
    "      \n",
    "          \n",
    "        # Attention computation\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask) #[B, n_h, L , d_h]\n",
    "\n",
    "        values = values.transpose(0, 2, 1, 3)  #  [B, L, n_h, d_h]\n",
    "        values = values.reshape(B, L, self.n_h * self.d_model // self.n_h) #[B, L, n_h * d_h]\n",
    "\n",
    "\n",
    "       \n",
    "        o = self.o_proj(values)  # [B, L, d_model]\n",
    "\n",
    "        return o, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    dim = 512  # Output dimension\n",
    "    num_heads = 8\n",
    "    latent_dim = 128  # Latent dimension (d_c)\n",
    "    seq_len = 64\n",
    "    batch_size = 2\n",
    "    \n",
    "    # d_model: int  # Output dimension (d_model)\n",
    "    # n_h: int  # Number of parallel heads (h)\n",
    "    # d_c : int # Latent dimension for compression (d_c) kv\n",
    "   \n",
    "    # Initialize model\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    model = MultiHeadLatentAttention(d_model=dim, n_h=num_heads, d_c=latent_dim )\n",
    "    x = jax.random.normal(rng, (batch_size, seq_len, dim))\n",
    "    params = model.init(rng, x)\n",
    "\n",
    "    # Forward pass\n",
    "    output, attention = model.apply(params, x)\n",
    "    print(\"Output shape:\", output.shape)  # Should be [batch_size, seq_len, dim]\n",
    "    print(\"Attention shape:\", attention.shape)  # Should be [batch_size, num_heads, seq_len, seq_len]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "Output shape: (2, 64, 512)\n",
      "Attention shape: (2, 8, 64, 64)\n",
      "48 ms ± 9.42 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
