{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "# Seeding for random operations\n",
    "main_rng = random.PRNGKey(42)\n",
    "\n",
    "import time\n",
    "\n",
    "from typing import Optional, List\n",
    "from einops import rearrange\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"D:/Malky/research/saved_models/mla_hybridNorm_jax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotary_embedding(x, max_seq_len, dim):\n",
    "    \"\"\"Apply RoPE to input tensor x.\"\"\"\n",
    "    # Ensure max_seq_len is a concrete value, not symbolic\n",
    "    positions = jnp.arange(max_seq_len, dtype=jnp.float32)\n",
    "    \n",
    "    # Ensure freqs is calculated with concrete values for dim\n",
    "    freqs = 1.0 / (10000 ** (jnp.arange(0, dim, 2, dtype=jnp.float32) / dim))\n",
    "    \n",
    "    # Angle calculation with fixed values\n",
    "    angles = positions[:, None] * freqs[None, :]\n",
    "    \n",
    "    sin, cos = jnp.sin(angles), jnp.cos(angles)\n",
    "    \n",
    "    # Apply rotary embeddings to the input tensor x\n",
    "    x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "    x_rotated = jnp.concatenate([x1 * cos - x2 * sin, x1 * sin + x2 * cos], axis=-1)\n",
    "    \n",
    "    return x_rotated\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "\n",
    "@jax.jit\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    \"\"\"Computes the scaled dot-product attention.\"\"\"\n",
    "    # q -> [B, n_h, L, d_h + d_h^R] \n",
    "    # k -> [B, n_h, L, d_h + d_h^R] \n",
    "    scale = jnp.sqrt(q.shape[-1])  # Scaling factor for attention scores  d_h + d_h_r\n",
    "    scores = jnp.einsum(\"bhqd,bhkd->bhqk\", q, k) / scale  # Efficient batch matmul\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores += mask  # Apply mask in-place\n",
    "\n",
    "    attention = nn.softmax(scores, axis=-1)  # Compute attention weights\n",
    "    values = jnp.einsum(\"bhqk,bhkd->bhqd\", attention, v)  # Apply attention to values\n",
    "\n",
    "    return values, attention\n",
    "\n",
    "# Helper function to support different mask shapes.\n",
    "# Output shape supports (B, number of heads, seq length, seq length)\n",
    "# If 2D: broadcasted over batch size and number of heads\n",
    "# If 3D: broadcasted over number of heads\n",
    "# If 4D: leave as is\n",
    "@jax.jit\n",
    "def expand_mask(mask):\n",
    "    \"\"\"Expands a mask tensor to shape (B, num_heads, L, L).\"\"\"\n",
    "    ndim = mask.ndim\n",
    "    assert ndim >= 2, \"Mask must be at least 2D (L x L)\"\n",
    "\n",
    "    # Efficient broadcasting using jnp.reshape and jnp.expand_dims\n",
    "    if ndim == 2:  # (L, L) → (1, 1, L, L)\n",
    "        return mask[None, None, :, :]\n",
    "    elif ndim == 3:  # (B, L, L) → (B, 1, L, L)\n",
    "        return mask[:, None, :, :]\n",
    "    \n",
    "    return mask  # If already (B, num_heads, L, L), return as is\n",
    "\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    d_model: int  # Output dimension\n",
    "    n_h: int  # Number of heads\n",
    "    d_c: int  # Latent compression dimension\n",
    "    d_c_: int #Latent caompression dimension fro Q\n",
    "\n",
    "    def setup(self):\n",
    "        #self.d_h = self.d_model // self.n_h  # Head dimension\n",
    "\n",
    "        #RmsNorm after latent variable \n",
    "        self.rmsNormV = nn.RMSNorm()\n",
    "        self.rmsNormK = nn.RMSNorm()\n",
    "        self.rmsNormQ = nn.RMSNorm()\n",
    "\n",
    "\n",
    "       # Fused projection layers for efficiency\n",
    "        self.kv_proj = nn.Dense(2 * self.d_c, kernel_init=nn.initializers.xavier_uniform(), bias_init=nn.initializers.zeros)\n",
    "        self.q_proj = nn.Dense(self.d_c_, kernel_init=nn.initializers.xavier_uniform(), bias_init=nn.initializers.zeros)\n",
    "\n",
    "        # Up-projection for keys, values, and queries\n",
    "        self.ukv_proj = nn.Dense(2 * self.d_model, kernel_init=nn.initializers.xavier_uniform(), bias_init=nn.initializers.zeros)\n",
    "        self.uq_proj = nn.Dense(self.d_model, kernel_init=nn.initializers.xavier_uniform(), bias_init=nn.initializers.zeros)\n",
    "\n",
    "        # Output projection\n",
    "        self.o_proj = nn.Dense(self.d_model, kernel_init=nn.initializers.xavier_uniform(), bias_init=nn.initializers.zeros) \n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        # B, L, _ = x.shape  # Batch size, Sequence length, Embedding dim\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = expand_mask(mask)\n",
    "\n",
    "        # Compute compressed KV projection\n",
    "        c_kv = self.kv_proj(x)  # [B, L, 2 * d_c]\n",
    "        k, v = jnp.split(self.ukv_proj(c_kv), 2, axis=-1)  # [B, L, d_model] each\n",
    "\n",
    "        # Compute query projection\n",
    "        q = self.uq_proj(self.q_proj(x))  # [B, L, d_model]\n",
    "\n",
    "        # Reshape using einops for efficiency\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b l (h d) -> b h l d\", h=self.n_h), [q, k, v])\n",
    "\n",
    "        q = self.rmsNormQ(q)\n",
    "        k = self.rmsNormK(k)\n",
    "        v = self.rmsNormV(v)\n",
    "        # Compute attention (optimized version)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        # Reshape and output projection\n",
    "        values = rearrange(values, \"b h l d -> b l (h d)\")\n",
    "        return self.o_proj(values), attention\n",
    "    \n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A Transformer block with RMS Norm and residual connections.\"\"\"\n",
    "    d_model: int  # Dimension of the model (hidden size)\n",
    "    n_h: int  # Number of attention heads\n",
    "    dim_feedforward: int    # Dimension of the feed-forward network\n",
    "    dropout_rate: float = 0.1  # Dropout rate\n",
    "    d_c : int = 64 # Latent dimension (d_c)\n",
    "    d_c_: int = 64 # Latent dimension (d_c)\n",
    "    d_h_R: int = 32  # Rotated dimension (d_h^R)\n",
    "    position: bool = False\n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        # Attention layer\n",
    "        if self.position:\n",
    "            self.self_attn = MultiHeadLatentAttentionRope(\n",
    "                d_model = self.d_model,\n",
    "                n_h = self.n_h,\n",
    "                d_c = self.d_c,\n",
    "                d_c_ = self.d_c_,\n",
    "                d_h_R = self.d_h_R)\n",
    "        else:\n",
    "            self.self_attn = MultiHeadLatentAttention(\n",
    "                d_model = self.d_model,\n",
    "                n_h = self.n_h,\n",
    "                d_c = self.d_c,\n",
    "                d_c_ = self.d_c_)\n",
    "            \n",
    "\n",
    "        # Feed-Forward Network\n",
    "        self.ffn = [\n",
    "            nn.Dense(features=self.dim_feedforward,\n",
    "                     kernel_init=nn.initializers.xavier_uniform(),\n",
    "                     bias_init=nn.initializers.zeros),\n",
    "            nn.gelu,\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Dense(features=self.d_model,\n",
    "                     kernel_init=nn.initializers.xavier_uniform(),\n",
    "                     bias_init=nn.initializers.zeros),\n",
    "            nn.Dropout(self.dropout_rate)\n",
    "        ]\n",
    "            \n",
    "         \n",
    "        # Layers to apply in between the main layers\n",
    "        # self.rmsNorm1 = nn.RMSNorm()\n",
    "        self.rmsNorm2 = nn.RMSNorm()\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray, mask: Optional[jnp.ndarray] = None, train: bool = False) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, length, d_model)\n",
    "            mask: Attention mask of shape (batch, heads, length, length) or None\n",
    "            train: Whether in training mode (for dropout)\n",
    "        Returns:\n",
    "            Output tensor of shape (batch, length, d_model)\n",
    "        \"\"\"\n",
    "        # First RMS Norm + Residual Connection\n",
    "        residual = x\n",
    "        # x = self.rmsNorm1(x)\n",
    "        x, _= self.self_attn(x, mask=mask)\n",
    "        x = self.dropout(x, deterministic=not train)\n",
    "        x = x + residual  # Residual connection\n",
    "\n",
    "        # Second RMS Norm + Residual Connection\n",
    "        \n",
    "        x = self.rmsNorm2(x)\n",
    "        x = self.dropout(x, deterministic=not train)\n",
    "\n",
    "        residual = x\n",
    "        \n",
    "        for layer in self.ffn:\n",
    "            x = layer(x) if not isinstance(layer, nn.Dropout) else layer(x, deterministic=not train)\n",
    "\n",
    "        x = self.dropout(x, deterministic=not train)\n",
    "        x = x + residual  # Residual connection\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    num_layers : int\n",
    "    d_model: int  # Dimension of the model (hidden size)\n",
    "    n_h: int  # Number of attention heads\n",
    "    dim_feedforward: int    # Dimension of the feed-forward network\n",
    "    dropout_rate: float = 0.1  # Dropout rate\n",
    "    d_c : int = 64 # Latent dimension (d_c)\n",
    "    d_c_: int = 64 # Latent dimension (d_c)\n",
    "    d_h_R: int = 32  # Rotated dimension (d_h^R)\n",
    "    position: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        # Initialize a list of Transformer blocks\n",
    "        self.layers = [TransformerBlock(\n",
    "            d_model=self.d_model,\n",
    "            n_h=self.n_h,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            d_c=self.d_c,\n",
    "            d_c_=self.d_c_,\n",
    "            d_h_R=self.d_h_R,\n",
    "            position=self.position\n",
    "            ) for _ in range(self.num_layers)]\n",
    "\n",
    "\n",
    "    def __call__(self, x:jnp.ndarray, mask:Optional[jnp.ndarray] = None, train:bool=True)-> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, length, d_model)\n",
    "            mask: Attention mask of shape (batch, heads, length, length) or None\n",
    "            train: Whether in training mode (for dropout)\n",
    "        Returns:\n",
    "            Output tensor of shape (batch, length, d_model)\n",
    "        \"\"\"\n",
    "        # Apply each Transformer block in sequence\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask, train=train)\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x:jnp.ndarray, mask:Optional[jnp.ndarray] = None, train:bool=True)-> List[jnp.ndarray]:\n",
    "        # A function to return the attention maps within the model for a single application\n",
    "        # Used for visualization purpose later\n",
    "        attention_maps = []\n",
    "        for layer in self.layers:\n",
    "    \n",
    "            _, attn_map = layer.self_attn(x, mask=mask)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = layer(x, mask=mask, train=train)\n",
    "        return attention_maps\n",
    "    \n",
    "\n",
    "\n",
    "class TransformerPredictor(nn.Module):\n",
    "    \n",
    "    num_classes : int                 # Number of classes to predict per sequence element\n",
    "    num_layers : int\n",
    "    d_model: int  # Dimension of the model (hidden size)\n",
    "    n_h: int  # Number of attention heads\n",
    "    dim_feedforward: int    # Dimension of the feed-forward network\n",
    "    dropout_rate: float = 0.1  # Dropout rate\n",
    "    input_dropout_prob : float = 0.0  # Dropout to apply on the input features\n",
    "    d_c : int = 64 # Latent dimension (d_c)\n",
    "    d_c_: int = 64 # Latent dimension (d_c)\n",
    "    d_h_R: int = 32  # Rotated dimension (d_h^R)\n",
    "    position: bool= False\n",
    "\n",
    "    def setup(self):\n",
    "        # Input dim -> Model dim\n",
    "        self.input_dropout = nn.Dropout(self.input_dropout_prob)\n",
    "        self.input_layer = nn.Dense(self.d_model)\n",
    "\n",
    "        # Transformer encoder\n",
    "        self.transformer = TransformerEncoder(\n",
    "            num_layers=self.num_layers,\n",
    "            d_model=self.d_model,\n",
    "            n_h=self.n_h,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            d_c=self.d_c,\n",
    "            d_c_=self.d_c_,\n",
    "            d_h_R=self.d_h_R,\n",
    "            position=self.position\n",
    "        )\n",
    "\n",
    "        # Output classifier per sequence element\n",
    "        self.output_net = [\n",
    "            nn.Dense(self.d_model),\n",
    "            nn.LayerNorm(),\n",
    "            nn.relu,\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Dense(self.num_classes)\n",
    "        ]\n",
    "\n",
    "\n",
    "    def __call__(self, x:jnp.ndarray, mask:Optional[jnp.ndarray] = None, train:bool=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features of shape [Batch, SeqLen, input_dim]\n",
    "            mask - Mask to apply on the attention outputs (optional)\n",
    "            add_positional_encoding - If True, we add the positional encoding to the input.\n",
    "                                      Might not be desired for some tasks.\n",
    "            train - If True, dropout is stochastic\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply input dropout and linear transformation\n",
    "        x = self.input_dropout(x, deterministic=not train)\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Apply Transformer encoder\n",
    "        x = self.transformer(x, mask=mask, train=train)\n",
    "\n",
    "\n",
    " \n",
    "        for layer in self.output_net:\n",
    "            x = layer(x) if not isinstance(layer, nn.Dropout) else layer(x, deterministic=not train)\n",
    "        return x\n",
    "        \n",
    "\n",
    "    def get_attention_maps(self, x:jnp.ndarray, mask:Optional[jnp.ndarray] = None, train:bool=True):\n",
    "        \"\"\"\n",
    "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
    "        Input arguments same as the forward pass.\n",
    "        \"\"\"\n",
    "        # Apply input dropout and linear transformation\n",
    "        x = self.input_dropout(x, deterministic=not train)\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Get attention maps from the Transformer encoder\n",
    "        attention_maps = self.transformer.get_attention_maps(x, mask=mask, train=train)\n",
    "        return attention_maps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(checkpoint_path: str, state):\n",
    "    # Load model. We use different checkpoint for the pretrained model\n",
    "    params = checkpoints.restore_checkpoint(ckpt_dir= checkpoint_path, target=state.params)\n",
    "    return params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, params, x, dropout_init_rng, mask=None, n_runs=10 ):\n",
    "    \"\"\"Benchmark the execution time of a Flax model.\"\"\"\n",
    "\n",
    "\n",
    "    # JIT compile the function\n",
    "    model_fn = jax.jit(lambda params, x, mask: model.apply({'params': params}, x, mask, train = False, rngs={'dropout': dropout_init_rng}))\n",
    "    # Warmup\n",
    "    model_fn(params, x, mask)\n",
    "\n",
    "    # Timing execution\n",
    "    start = time.time()\n",
    "    for _ in range(n_runs):\n",
    "        _ = model_fn(params, x, mask)\n",
    "    end = time.time()\n",
    "\n",
    "    avg_time = (end - start) / n_runs\n",
    "    return avg_time\n",
    "\n",
    "def compute_flops(model, params, x, dropout_init_rng, mask=None):\n",
    "    \"\"\"Compute an approximate count of FLOPs for a Flax model.\"\"\"\n",
    "  \n",
    "    # JAX tracing to get Jaxpr (computational graph)\n",
    "    jaxpr = jax.make_jaxpr(lambda x, mask: model.apply(\n",
    "        {'params': params}, x, mask, train=False, rngs={'dropout': dropout_init_rng}))(x, mask)\n",
    "\n",
    "    # Count the number of floating-point operations\n",
    "    flops = sum(eq.primitive.name in [\"dot_general\", \"conv_general_dilated\"] for eq in jaxpr.jaxpr.eqns)\n",
    "\n",
    "    return flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 128\n",
    "n_h = 4\n",
    "d_h = d_model//n_h\n",
    "d_c = d_h // 2\n",
    "d_c_ = d_h // 4\n",
    "\n",
    "model_kwargs = {\n",
    "    \"d_model\":d_model,\n",
    "    \"n_h\":n_h,\n",
    "    \"num_classes\":1,\n",
    "    \"num_layers\":4,\n",
    "    \"dim_feedforward\" : 256,\n",
    "    \"dropout_rate\":0.1,\n",
    "    \"input_dropout_prob\":0.1,\n",
    "    \"d_c\" : d_c,\n",
    "    \"d_c_\": d_c_,\n",
    "    \"position\": False\n",
    "}\n",
    "model = TransformerPredictor(**model_kwargs)\n",
    "\n",
    "main_rng, x_rng = random.split(main_rng)\n",
    "x = random.normal(x_rng, (64,10,512)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(423)\n",
    "rng, init_rng, dropout_init_rng = jax.random.split(rng, 3)\n",
    "params = model.init({'params': init_rng, 'dropout': dropout_init_rng}, x, train=True)['params']\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
    "\n",
    "params = load_model(CHECKPOINT_PATH, state=state )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_time_mla = benchmark_model(model=model,params=params, x=x, dropout_init_rng=dropout_init_rng)\n",
    "flops_mla = compute_flops(model=model,params=params, x=x, dropout_init_rng=dropout_init_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadLatentAttention Time: 0.000100 sec\n",
      "MultiHeadLatentAttention FLOPs: 31\n"
     ]
    }
   ],
   "source": [
    "print(f\"MultiHeadLatentAttention Time: {execution_time_mla:.6f} sec\")\n",
    "print(f\"MultiHeadLatentAttention FLOPs: {flops_mla}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
