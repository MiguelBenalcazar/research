{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional, Literal\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "\n",
    "from einops import rearrange\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"D:/Malky/research/data/mla_jax\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"D:/Malky/research/saved_models/mla_deepSeek_reverse\"\n",
    "\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
    "    # \n",
    "    if input_data is not None:\n",
    "        input_data = input_data[idx].detach().cpu().numpy()\n",
    "    else:\n",
    "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
    "    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n",
    "\n",
    "    num_heads = attn_maps[0].shape[0]\n",
    "    num_layers = len(attn_maps)\n",
    "    seq_len = input_data.shape[0]\n",
    "    fig_size = 4 if num_heads == 1 else 3\n",
    "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
    "    if num_layers == 1:\n",
    "        ax = [ax]\n",
    "    if num_heads == 1:\n",
    "        ax = [[a] for a in ax]\n",
    "    for row in range(num_layers):\n",
    "        for column in range(num_heads):\n",
    "            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n",
    "            ax[row][column].set_xticks(list(range(seq_len)))\n",
    "            # ax[row][column].set_xticklabels(input_data.tolist())\n",
    "            ax[row][column].set_yticks(list(range(seq_len)))\n",
    "            # ax[row][column].set_yticklabels(input_data.tolist())\n",
    "            ax[row][column].set_title(f\"Layer {row+1}, Head {column+1}\")\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 1\n",
    "rank = 0\n",
    "block_size = 128\n",
    "gemm_impl: Literal[\"bf16\", \"fp8\"] = \"bf16\"\n",
    "attn_impl: Literal[\"naive\", \"absorb\"] = \"naive\" # naive -> train, absorb -> inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    \"\"\"\n",
    "    Data class for defining model arguments and hyperparameters.\n",
    "\n",
    "    Attributes:\n",
    "        max_batch_size (int): Maximum batch size.\n",
    "        max_seq_len (int): Maximum sequence length.\n",
    "        dtype (Literal[\"bf16\", \"fp8\"]): Data type for computations.\n",
    "        vocab_size (int): Vocabulary size.\n",
    "        dim (int): Model dimension.\n",
    "        inter_dim (int): Intermediate dimension for MLP layers.\n",
    "        moe_inter_dim (int): Intermediate dimension for MoE layers.\n",
    "        n_layers (int): Number of transformer layers.\n",
    "        n_dense_layers (int): Number of dense layers in the model.\n",
    "        n_heads (int): Number of attention heads.\n",
    "        n_routed_experts (int): Number of routed experts for MoE layers.\n",
    "        n_shared_experts (int): Number of shared experts for MoE layers.\n",
    "        n_activated_experts (int): Number of activated experts in MoE layers.\n",
    "        n_expert_groups (int): Number of expert groups.\n",
    "        n_limited_groups (int): Number of limited groups for MoE routing.\n",
    "        score_func (Literal[\"softmax\", \"sigmoid\"]): Scoring function for MoE routing.\n",
    "        route_scale (float): Scaling factor for routing scores.\n",
    "        q_lora_rank (int): LoRA rank for query projections.\n",
    "        kv_lora_rank (int): LoRA rank for key-value projections.\n",
    "        qk_nope_head_dim (int): Dimension for query-key projections without positional embeddings. size of the future predictions\n",
    "        qk_rope_head_dim (int): Dimension for query-key projections with rotary embeddings.\n",
    "        v_head_dim (int): Dimension for value projections.\n",
    "        original_seq_len (int): Original sequence length.\n",
    "        rope_theta (float): Base for rotary positional encoding.\n",
    "        rope_factor (float): Scaling factor for extended sequence lengths.\n",
    "        beta_fast (int): Fast beta correction factor.\n",
    "        beta_slow (int): Slow beta correction factor.\n",
    "        mscale (float): Scaling factor for extended attention.\n",
    "    \"\"\"\n",
    "    max_batch_size: int = 1 # 8\n",
    "    max_seq_len: int = 128 # 4096 * 4\n",
    "    dtype: Literal[\"bf16\", \"fp8\"] = \"bf16\"\n",
    "    vocab_size: int = 10#102400 \n",
    "    dim: int = 64 #2048\n",
    "    inter_dim: int = 128# 10944\n",
    "    # moe_inter_dim: int = 1408\n",
    "    n_layers: int = 1#27\n",
    "    n_dense_layers: int = 1\n",
    "    n_heads: int = 1#16\n",
    "    # moe\n",
    "    # n_routed_experts: int = 64\n",
    "    # n_shared_experts: int = 2\n",
    "    # n_activated_experts: int = 6\n",
    "    # n_expert_groups: int = 1\n",
    "    # n_limited_groups: int = 1\n",
    "    # score_func: Literal[\"softmax\", \"sigmoid\"] = \"softmax\"\n",
    "    # route_scale: float = 1.\n",
    "    # mla\n",
    "    q_lora_rank: int = 0\n",
    "    kv_lora_rank: int = 64 #512\n",
    "    qk_nope_head_dim: int = 32# 128\n",
    "    qk_rope_head_dim: int = 16# 64\n",
    "    v_head_dim: int = 64 #128\n",
    "    \n",
    "    # yarn\n",
    "    original_seq_len: int = 128 #4096\n",
    "    rope_theta: float = 10000.0\n",
    "    rope_factor: float = 40\n",
    "    beta_fast: int = 32\n",
    "    beta_slow: int = 1\n",
    "    mscale: float = 1.\n",
    "\n",
    "\n",
    "    input_dim:int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies rotary positional embeddings to the input tensor.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor with positional embeddings to be applied.\n",
    "        freqs_cis (torch.Tensor): Precomputed complex exponential values for positional embeddings.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor with rotary embeddings applied.\n",
    "    \"\"\"\n",
    "    dtype = x.dtype\n",
    "    x = torch.view_as_complex(x.float().view(*x.shape[:-1], -1, 2))\n",
    "    freqs_cis = freqs_cis.view(1, x.size(1), 1, x.size(-1))\n",
    "    y = torch.view_as_real(x * freqs_cis).flatten(3)\n",
    "    return y.to(dtype)\n",
    "\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(args: ModelArgs) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Precomputes frequency-based complex exponential values for rotary positional embeddings.\n",
    "\n",
    "    Args:\n",
    "        args (ModelArgs): Model arguments containing positional embedding parameters.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Precomputed complex exponential values for positional embeddings.\n",
    "    \"\"\"\n",
    "    dim = args.qk_rope_head_dim\n",
    "    seqlen = args.max_seq_len\n",
    "    beta_fast = args.beta_fast\n",
    "    beta_slow = args.beta_slow\n",
    "    base = args.rope_theta\n",
    "    factor = args.rope_factor\n",
    "\n",
    "    def find_correction_dim(num_rotations, dim, base, max_seq_len):\n",
    "        \"\"\"\n",
    "        Computes the correction dimension for a given number of rotations in the rotary positional embedding.\n",
    "\n",
    "        Args:\n",
    "            num_rotations (float): Number of rotations to compute the correction for.\n",
    "            dim (int): Dimensionality of the embedding space.\n",
    "            base (float): Base value for the exponential computation.\n",
    "            max_seq_len (int): Maximum sequence length.\n",
    "\n",
    "        Returns:\n",
    "            float: The correction dimension based on the input parameters.\n",
    "        \"\"\"\n",
    "        return dim * math.log(max_seq_len / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n",
    "\n",
    "    def find_correction_range(low_rot, high_rot, dim, base, max_seq_len):\n",
    "        \"\"\"\n",
    "        Computes the range of correction dimensions for rotary positional embeddings.\n",
    "\n",
    "        Args:\n",
    "            low_rot (float): Lower bound for the number of rotations.\n",
    "            high_rot (float): Upper bound for the number of rotations.\n",
    "            dim (int): Dimensionality of the embedding space.\n",
    "            base (float): Base value for the exponential computation.\n",
    "            max_seq_len (int): Maximum sequence length.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, int]: The range of correction dimensions (low, high), clamped to valid indices.\n",
    "        \"\"\"\n",
    "        low = math.floor(find_correction_dim(low_rot, dim, base, max_seq_len))\n",
    "        high = math.ceil(find_correction_dim(high_rot, dim, base, max_seq_len))\n",
    "        return max(low, 0), min(high, dim-1)\n",
    "\n",
    "    def linear_ramp_factor(min, max, dim):\n",
    "        \"\"\"\n",
    "        Computes a linear ramp function used to smooth values between a minimum and maximum range.\n",
    "\n",
    "        Args:\n",
    "            min (float): Minimum value for the ramp function.\n",
    "            max (float): Maximum value for the ramp function.\n",
    "            dim (int): Dimensionality of the ramp tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (dim,) with values linearly interpolated between 0 and 1,\n",
    "                clamped to the range [0, 1].\n",
    "        \"\"\"\n",
    "        if min == max:\n",
    "            max += 0.001\n",
    "        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
    "        ramp_func = torch.clamp(linear_func, 0, 1)\n",
    "        return ramp_func\n",
    "\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
    "    if seqlen > args.original_seq_len:\n",
    "        low, high = find_correction_range(beta_fast, beta_slow, dim, base, args.original_seq_len)\n",
    "        smooth = 1 - linear_ramp_factor(low, high, dim // 2)\n",
    "        freqs = freqs / factor * (1 - smooth) + freqs * smooth\n",
    "\n",
    "    t = torch.arange(seqlen)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLAPos(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Headed Attention Layer (MLA).\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): Dimensionality of the input features.\n",
    "        n_heads (int): Number of attention heads.\n",
    "        n_local_heads (int): Number of local attention heads for distributed systems.\n",
    "        q_lora_rank (int): Rank for low-rank query projection.\n",
    "        kv_lora_rank (int): Rank for low-rank key/value projection.\n",
    "        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.\n",
    "        qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.\n",
    "        qk_head_dim (int): Total dimensionality of query/key projections.\n",
    "        v_head_dim (int): Dimensionality of value projections.\n",
    "        softmax_scale (float): Scaling factor for softmax in attention computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.dim = args.dim\n",
    "        self.n_heads = args.n_heads\n",
    "        self.n_local_heads = args.n_heads // world_size\n",
    "        self.q_lora_rank = args.q_lora_rank\n",
    "        self.kv_lora_rank = args.kv_lora_rank\n",
    "        self.qk_nope_head_dim = args.qk_nope_head_dim\n",
    "        self.qk_rope_head_dim = args.qk_rope_head_dim\n",
    "        self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim\n",
    "        self.v_head_dim = args.v_head_dim\n",
    "\n",
    "        # Query\n",
    "        if self.q_lora_rank == 0:\n",
    "            self.wq = torch.nn.Linear(in_features=self.dim, out_features= self.n_heads * self.qk_head_dim) # normal Query (no latent space)\n",
    "        else:\n",
    "            self.wq_a = torch.nn.Linear(in_features = self.dim, out_features = self.q_lora_rank)  # latent space (Q reduced )\n",
    "            self.q_norm = torch.nn.RMSNorm(normalized_shape = self.q_lora_rank)\n",
    "            self.wq_b = torch.nn.Linear(in_features= self.q_lora_rank, out_features=self.n_heads * self.qk_head_dim)\n",
    "        \n",
    "        self.wkv_a = torch.nn.Linear(in_features=self.dim, out_features=self.kv_lora_rank + self.qk_rope_head_dim)\n",
    "        self.kv_norm = torch.nn.RMSNorm(normalized_shape =self.kv_lora_rank)\n",
    "        self.wkv_b = torch.nn.Linear(in_features=self.kv_lora_rank, out_features=self.n_heads * (self.qk_nope_head_dim + self.v_head_dim))\n",
    "\n",
    "\n",
    "\n",
    "        self.wo = torch.nn.Linear(in_features=self.n_heads * self.v_head_dim, out_features= self.dim)\n",
    "        \n",
    "        self.softmax_scale = self.qk_head_dim ** -0.5\n",
    "        if args.max_seq_len > args.original_seq_len:\n",
    "            mscale = 0.1 * args.mscale * math.log(args.rope_factor) + 1.0\n",
    "            self.softmax_scale = self.softmax_scale * mscale * mscale\n",
    "\n",
    "        if attn_impl == \"naive\":\n",
    "            self.register_buffer(\"k_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.qk_head_dim), persistent=False)\n",
    "            self.register_buffer(\"v_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.v_head_dim), persistent=False)\n",
    "            \n",
    "            # Init parameters\n",
    "            self._reset_parameters() \n",
    "        else:\n",
    "            self.register_buffer(\"kv_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.kv_lora_rank), persistent=False)\n",
    "            self.register_buffer(\"pe_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.qk_rope_head_dim), persistent=False)\n",
    "            \n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "\n",
    "        if self.q_lora_rank == 0:\n",
    "            nn.init.xavier_uniform_(self.wq.weight)\n",
    "            self.wq.bias.data.fill_(0)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.wq_a.weight)\n",
    "            self.wq_a.bias.data.fill_(0)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.wq_b.weight)\n",
    "            self.wq_b.bias.data.fill_(0)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.wkv_a.weight)\n",
    "        self.wkv_a.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.wkv_b.weight)\n",
    "        self.wkv_b.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.wo.weight)\n",
    "        self.wo.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor], return_attention: bool= False):\n",
    "        \"\"\"\n",
    "        Forward pass for the Multi-Headed Attention Layer (MLA).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).\n",
    "            start_pos (int): Starting position in the sequence for caching.\n",
    "            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n",
    "            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with the same shape as the input.\n",
    "        \"\"\"\n",
    "        bsz, seqlen, _ = x.size()\n",
    "        end_pos = start_pos + seqlen\n",
    "        \n",
    "        # Queries\n",
    "        if self.q_lora_rank == 0:\n",
    "            q = self.wq(x)\n",
    "        \n",
    "        else:\n",
    "            # reduction (latent space) -> norm -> increse space \n",
    "            q = self.wq_b(self.q_norm(self.wq_a(x)))\n",
    "\n",
    "        q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim) # batch, sequence lenght, number of heads, dimension\n",
    "        \n",
    "        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n",
    "\n",
    "        # KV -> keys , values\n",
    "        kv = self.wkv_a(x) # get kv reduction\n",
    "        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n",
    "        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)\n",
    "\n",
    "        \n",
    "        if attn_impl == \"naive\":\n",
    "            q = torch.cat([q_nope, q_pe], dim=-1) # concatenate [q^C, q^R]\n",
    "            kv = self.wkv_b(self.kv_norm(kv)) # increase dimension\n",
    "            kv = kv.view(bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim) # batch, sequence lenght, number of heads, dimension\n",
    "            k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1) # k^C, v^C\n",
    "            k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_local_heads, -1)], dim=-1) # concat[k^c, k^R]\n",
    "            self.k_cache[:bsz, start_pos:end_pos] = k\n",
    "            self.v_cache[:bsz, start_pos:end_pos] = v\n",
    "            scores = torch.einsum(\"bshd,bthd->bsht\", q, self.k_cache[:bsz, :end_pos]) * self.softmax_scale\n",
    "        else:\n",
    "            wkv_b = self.wkv_b.weight\n",
    "            wkv_b = wkv_b.view(self.n_local_heads, -1, self.kv_lora_rank)\n",
    "            q_nope = torch.einsum(\"bshd,hdc->bshc\", q_nope, wkv_b[:, :self.qk_nope_head_dim])\n",
    "            self.kv_cache[:bsz, start_pos:end_pos] = self.kv_norm(kv)\n",
    "            self.pe_cache[:bsz, start_pos:end_pos] = k_pe.squeeze(2)\n",
    "            scores = (torch.einsum(\"bshc,btc->bsht\", q_nope, self.kv_cache[:bsz, :end_pos]) +\n",
    "                      torch.einsum(\"bshr,btr->bsht\", q_pe, self.pe_cache[:bsz, :end_pos])) * self.softmax_scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores += mask.unsqueeze(1)\n",
    "        scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x)\n",
    "        \n",
    "        \n",
    "        if attn_impl == \"naive\":\n",
    "            x = torch.einsum(\"bsht,bthd->bshd\", scores, self.v_cache[:bsz, :end_pos])\n",
    "        else:\n",
    "            x = torch.einsum(\"bsht,btc->bshc\", scores, self.kv_cache[:bsz, :end_pos])\n",
    "            x = torch.einsum(\"bshc,hdc->bshd\", x, wkv_b[:, -self.v_head_dim:])\n",
    "   \n",
    "        x = self.wo(x.flatten(2))\n",
    "\n",
    "        if return_attention:\n",
    "            return x, scores\n",
    "        else:\n",
    "            return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edited MLA with no Rotary position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLA(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Headed Attention Layer (MLA).\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): Dimensionality of the input features.\n",
    "        n_heads (int): Number of attention heads.\n",
    "        n_local_heads (int): Number of local attention heads for distributed systems.\n",
    "        q_lora_rank (int): Rank for low-rank query projection.\n",
    "        kv_lora_rank (int): Rank for low-rank key/value projection.\n",
    "        qk_head_dim (int): Total dimensionality of query/key projections. \n",
    "        v_head_dim (int): Dimensionality of value projections.\n",
    "        softmax_scale (float): Scaling factor for softmax in attention computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.dim = args.dim\n",
    "        self.n_heads = args.n_heads\n",
    "        self.n_local_heads = args.n_heads // world_size\n",
    "        self.q_lora_rank = args.q_lora_rank\n",
    "        self.kv_lora_rank = args.kv_lora_rank\n",
    "        self.qk_head_dim = args.qk_nope_head_dim \n",
    "        self.v_head_dim = args.v_head_dim\n",
    "\n",
    "        # Query\n",
    "        if self.q_lora_rank == 0:\n",
    "            self.wq = torch.nn.Linear(in_features=self.dim, out_features= self.n_heads * self.qk_head_dim) # normal Query (no latent space)\n",
    "        else:\n",
    "            self.wq_a = torch.nn.Linear(in_features = self.dim, out_features = self.q_lora_rank)  # latent space (Q reduced )\n",
    "            self.q_norm = torch.nn.RMSNorm(normalized_shape = self.q_lora_rank)\n",
    "            self.wq_b = torch.nn.Linear(in_features= self.q_lora_rank, out_features=self.n_heads * self.qk_head_dim)\n",
    "        \n",
    "        self.wkv_a = torch.nn.Linear(in_features=self.dim, out_features=self.kv_lora_rank)\n",
    "        self.kv_norm = torch.nn.RMSNorm(normalized_shape =self.kv_lora_rank)\n",
    "        self.wkv_b = torch.nn.Linear(in_features=self.kv_lora_rank, out_features=self.n_heads * (self.qk_head_dim + self.v_head_dim))\n",
    "\n",
    "\n",
    "\n",
    "        self.wo = torch.nn.Linear(in_features=self.n_heads * self.v_head_dim, out_features= self.dim)\n",
    "        \n",
    "        self.softmax_scale = self.qk_head_dim ** -0.5\n",
    "\n",
    "        if attn_impl == \"naive\":\n",
    "            self.register_buffer(\"k_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.qk_head_dim), persistent=False)\n",
    "            self.register_buffer(\"v_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.v_head_dim), persistent=False)\n",
    "        else:\n",
    "            self.register_buffer(\"kv_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.kv_lora_rank), persistent=False)\n",
    "\n",
    "        # include K,Q,V\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "\n",
    "        if self.q_lora_rank == 0:\n",
    "            nn.init.xavier_uniform_(self.wq.weight)\n",
    "            self.wq.bias.data.fill_(0)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.wq_a.weight)\n",
    "            self.wq_a.bias.data.fill_(0)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.wq_b.weight)\n",
    "            self.wq_b.bias.data.fill_(0)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.wkv_a.weight)\n",
    "        self.wkv_a.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.wkv_b.weight)\n",
    "        self.wkv_b.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.wo.weight)\n",
    "        self.wo.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int,  mask: Optional[torch.Tensor],  return_attention: bool= False):\n",
    "        \"\"\"\n",
    "        Forward pass for the Multi-Headed Attention Layer (MLA).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).\n",
    "            start_pos (int): Starting position in the sequence for caching.\n",
    "            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with the same shape as the input.\n",
    "        \"\"\"\n",
    " \n",
    "        bsz, seqlen, _ = x.size()\n",
    "        end_pos = start_pos + seqlen\n",
    "        \n",
    "        # Queries\n",
    "        if self.q_lora_rank == 0:\n",
    "            q = self.wq(x)\n",
    "        else:\n",
    "            # reduction (latent space) -> norm -> increse space \n",
    "            q = self.wq_b(self.q_norm(self.wq_a(x)))\n",
    "\n",
    "        q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim) # batch, sequence lenght, number of heads, dimension\n",
    "        \n",
    "        # KV -> keys , values\n",
    "        kv = self.wkv_a(x) # get kv reduction\n",
    "       \n",
    "        if attn_impl == \"naive\":\n",
    "            kv = self.wkv_b(self.kv_norm(kv)) # increase dimension\n",
    "            kv = kv.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim + self.v_head_dim) # batch, sequence lenght, number of heads, dimension\n",
    "     \n",
    "            k, v = torch.split(kv, [self.qk_head_dim, self.v_head_dim], dim=-1) # k^C, v^C\n",
    "            \n",
    "            self.k_cache[:bsz, start_pos:end_pos] = k\n",
    "            self.v_cache[:bsz, start_pos:end_pos] = v\n",
    "            scores = torch.einsum(\"bshd,bthd->bsht\", q, self.k_cache[:bsz, :end_pos]) * self.softmax_scale\n",
    "        else:\n",
    "            \n",
    "            wkv_b = self.wkv_b.weight.view(self.n_local_heads, -1, self.kv_lora_rank) # n_local_heads x kv_lora x d\n",
    "            # Apply LoRA transformation to queries (previously q_nope)\n",
    "            q = torch.einsum(\"bshd,hdc->bshc\", q, wkv_b[:, :self.qk_head_dim])\n",
    "            \n",
    "            self.kv_cache[:bsz, start_pos:end_pos] = self.kv_norm(kv)\n",
    "            scores = torch.einsum(\"bshc,btc->bsht\", q, self.kv_cache[:bsz, :end_pos]) * self.softmax_scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores += mask.unsqueeze(1)\n",
    "        scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x) # score softmax of the scores and set the type as the same as the input x\n",
    "        \n",
    "        \n",
    "        if attn_impl == \"naive\":\n",
    "            x = torch.einsum(\"bsht,bthd->bshd\", scores, self.v_cache[:bsz, :end_pos])\n",
    "        else:\n",
    "            x = torch.einsum(\"bsht,btc->bshc\", scores, self.kv_cache[:bsz, :end_pos])\n",
    "            x = torch.einsum(\"bshc,hdc->bshd\", x, wkv_b[:, -self.v_head_dim:])\n",
    "   \n",
    "        x = self.wo(x.flatten(2))\n",
    "        \n",
    "       \n",
    "        if return_attention:\n",
    "            return x, scores\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 10, 64])\n",
      "attention  torch.Size([1, 10, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "attn_impl= \"absorb\"#Literal[\"naive\", \"absorb\"] = \"naive\" # naive -> train, absorb -> inference\n",
    "args = ModelArgs()\n",
    "mla = MLA(args)\n",
    "batch_size = 1\n",
    "seq_len = 10\n",
    "x= torch.randn(batch_size, seq_len, args.dim)\n",
    "output , attention = mla(x = x, start_pos =  0, mask = None, return_attention = True)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"attention \", attention.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (MLP) used as a feed-forward layer.\n",
    "\n",
    "    Attributes:\n",
    "        w1 (nn.Module): Linear layer for input-to-hidden transformation.\n",
    "        w2 (nn.Module): Linear layer for hidden-to-output transformation.\n",
    "        w3 (nn.Module): Additional linear layer for feature transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, inter_dim: int):\n",
    "        \"\"\"\n",
    "        Initializes the MLP layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Input and output dimensionality.\n",
    "            inter_dim (int): Hidden layer dimensionality.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w1 = torch.nn.Linear(in_features=dim, out_features=inter_dim)\n",
    "        self.w2 = torch.nn.Linear(in_features=inter_dim, out_features=dim)\n",
    "        self.w3 = torch.nn.Linear(in_features=dim, out_features=inter_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the MLP layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after MLP computation.\n",
    "        \"\"\"\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x)) #Output=W_2(SiLU(W_1x) ⊙ (W_3x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 12\n",
    "dim = 64\n",
    "\n",
    "ffn = MLP(dim=64, inter_dim=128)\n",
    "x = torch.randn(batch_size, seq_len, dim)\n",
    "result = ffn(x)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define block with position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockPos(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block combining attention and feed-forward layers.\n",
    "\n",
    "    Attributes:\n",
    "        attn (nn.Module): Attention layer (MLA).\n",
    "        ffn (nn.Module): Feed-forward network (MLP or MoE).\n",
    "        attn_norm (nn.Module): Layer normalization for attention.\n",
    "        ffn_norm (nn.Module): Layer normalization for feed-forward network.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            layer_id (int): Layer index in the transformer.\n",
    "            args (ModelArgs): Model arguments containing block parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attn = MLAPos(args)\n",
    "        self.ffn = MLP(args.dim, args.inter_dim) #if layer_id < args.n_dense_layers else MoE(args)\n",
    "        self.attn_norm = torch.nn.RMSNorm(args.dim)\n",
    "        self.ffn_norm = torch.nn.RMSNorm(args.dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            start_pos (int): Starting position in the sequence.\n",
    "            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n",
    "            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after block computation.\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.attn_norm(x), start_pos, freqs_cis, mask)\n",
    "        x = self.ffn_norm(x)\n",
    "        x = x + self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "class TransformerPos(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model with positional embeddings, multiple layers, and output projection.\n",
    "\n",
    "    Attributes:\n",
    "        max_seq_len (int): Maximum sequence length for the transformer.\n",
    "        embed (nn.Module): Embedding layer for input tokens.\n",
    "        layers (torch.nn.ModuleList): List of transformer blocks.\n",
    "        norm (nn.Module): Layer normalization applied after all blocks.\n",
    "        head (nn.Module): Output projection layer mapping to vocabulary size.\n",
    "        freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            args (ModelArgs): Model arguments containing transformer parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        # self.embed = torch.nn.Linear(args.vocab_size, args.dim)\n",
    "\n",
    "        print(self.max_seq_len, args.dim)\n",
    "\n",
    "        self.embed = torch.nn.Linear(in_features = args.input_dim, out_features= args.dim)\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(args.n_layers):\n",
    "            self.layers.append(BlockPos(layer_id, args))\n",
    "        # self.norm = torch.nn.RMSNorm(args.dim)\n",
    "        # self.head = torch.nn.Linear(args.dim, args.vocab_size, dtype=torch.get_default_dtype())\n",
    "        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(args), persistent=False)\n",
    "\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, mask: Optional[torch.Tensor], start_pos: int = 0):\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            tokens (torch.Tensor): Input tensor of token IDs with shape (batch_size, seq_len).\n",
    "            start_pos (int, optional): Starting position in the sequence for rotary embeddings. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits tensor of shape (batch_size, vocab_size).\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        seqlen = tokens.size(1)\n",
    "       \n",
    "        h = self.embed(tokens)\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos+seqlen]\n",
    " \n",
    "    \n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "\n",
    "    \n",
    "        # h = self.norm(h)[:, -1]\n",
    "        # h = self.norm(h)\n",
    "        # print(\"h print after norm\",h.shape)\n",
    "        # logits = self.head(h)\n",
    "\n",
    "        \n",
    "        # return logits\n",
    "\n",
    "        return h\n",
    "    \n",
    "\n",
    "    def get_attention_maps(self, tokens: torch.Tensor, mask=Optional[torch.Tensor], start_pos: int = 0):\n",
    "        seqlen = tokens.size(1)\n",
    "        h = self.embed(tokens)\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos+seqlen]\n",
    "\n",
    "        attention_maps = []\n",
    "        for layer in self.layers:\n",
    "            _, attn_map = layer.attn(x=h, start_pos = start_pos, freqs_cis =freqs_cis, mask = mask, return_attention = True)\n",
    "            attention_maps.append(rearrange(attn_map, \"b s h d -> b h s d\"))\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        return attention_maps\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(max_batch_size=1, max_seq_len=10, dtype='bf16', vocab_size=10, dim=64, inter_dim=128, n_layers=1, n_dense_layers=1, n_heads=1, q_lora_rank=0, kv_lora_rank=64, qk_nope_head_dim=32, qk_rope_head_dim=16, v_head_dim=64, original_seq_len=128, rope_theta=10000.0, rope_factor=40, beta_fast=32, beta_slow=1, mscale=1.0, input_dim=2)\n",
      "10 64\n",
      "Output shape: torch.Size([1, 10, 64])\n",
      "attention  torch.Size([1, 10, 1, 10])\n",
      "torch.Size([1, 1, 10, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_impl= \"absorb\"#Literal[\"naive\", \"absorb\"] = \"naive\" # naive -> train, absorb -> inference\n",
    "args = ModelArgs()\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 10\n",
    "x= torch.randn(batch_size, seq_len, 2)\n",
    "args.max_seq_len = seq_len\n",
    "\n",
    "print(args)\n",
    "\n",
    "transpos = TransformerPos(args)\n",
    "output  = transpos(tokens = x, mask = None, start_pos = 0)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"attention \", attention.shape)\n",
    "\n",
    "att = transpos.get_attention_maps(tokens = x, mask = None, start_pos = 0)\n",
    "for i in att:\n",
    "    print(i.shape)\n",
    "plot_attention_maps(x, att, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelArgs(max_batch_size=128, \n",
    "          max_seq_len=128, dtype='bf16',\n",
    "            vocab_size=10, \n",
    "            dim=64, \n",
    "            inter_dim=128, \n",
    "            n_layers=1, \n",
    "            n_dense_layers=1, \n",
    "            n_heads=1, \n",
    "            q_lora_rank=0, kv_lora_rank=64, qk_nope_head_dim=32, qk_rope_head_dim=16, \n",
    "          v_head_dim=64, original_seq_len=128, rope_theta=10000.0, rope_factor=40, beta_fast=32, beta_slow=1, mscale=1.0, input_dim=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Hidden dimensionality of the input\n",
    "            model_dim - Hidden dimensionality to use inside the Transformer\n",
    "            num_classes - Number of classes to predict per sequence element\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention blocks\n",
    "            num_layers - Number of encoder blocks to use.\n",
    "            lr - Learning rate in the optimizer\n",
    "            warmup - Number of warmup steps. Usually between 50 and 500\n",
    "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
    "            dropout - Dropout to apply inside the model\n",
    "            input_dropout - Dropout to apply on the input features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.transformer = TransformerPos(args=args)\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(args.dim, args.dim),\n",
    "            nn.LayerNorm(args.dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(args.dim, args.vocab_size)\n",
    "        )\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=Optional[torch.Tensor], start_pos: int = 0):\n",
    "        x = self.transformer(tokens = x, mask = mask, start_pos = start_pos)\n",
    "        x = self.output_net(x)\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_attention_maps(self, x: torch.Tensor, mask=Optional[torch.Tensor], start_pos: int = 0):\n",
    "        \"\"\"\n",
    "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
    "        Input arguments same as the forward pass.\n",
    "        \"\"\"\n",
    "        attention_maps = self.transformer.get_attention_maps(tokens = x, mask = mask, start_pos = start_pos)\n",
    "        return attention_maps\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "        # Apply lr scheduler per step\n",
    "        lr_scheduler = CosineWarmupScheduler(optimizer,\n",
    "                                             warmup=50,\n",
    "                                             max_iters=50)\n",
    "        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # #  return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        # optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
    "        \n",
    "        # # Apply lr scheduler per step\n",
    "        # warmup_epochs = 5\n",
    "        # total_epochs = 50\n",
    "\n",
    "        # warmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=warmup_epochs)\n",
    "        # cosine_scheduler = CosineAnnealingLR(optimizer, T_max=total_epochs - warmup_epochs)\n",
    "        # scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
    "\n",
    "        # return {\n",
    "        #     \"optimizer\": optimizer,\n",
    "        #     \"lr_scheduler\": {\n",
    "        #         \"scheduler\": scheduler,\n",
    "        #         \"interval\": \"step\"  # Adjust learning rate per step\n",
    "        #     }\n",
    "        # }\n",
    "       \n",
    "       \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "\n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = partial(ReverseDataset, 10, 16)\n",
    "train_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader   = data.DataLoader(dataset(1000), batch_size=128)\n",
    "test_loader  = data.DataLoader(dataset(10000), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: tensor([1, 1, 5, 8, 6, 8, 1, 6, 8, 6, 3, 5, 5, 8, 9, 9])\n",
      "Labels:     tensor([9, 9, 8, 5, 5, 3, 6, 8, 6, 1, 8, 6, 8, 5, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "inp_data, labels = train_loader.dataset[0]\n",
    "print(\"Input data:\", inp_data)\n",
    "print(\"Labels:    \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversePredictor(TransformerPredictor):\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        # Fetch data and transform categories to one-hot vectors\n",
    "       \n",
    "        inp_data, labels = batch\n",
    "        inp_data = F.one_hot(inp_data, num_classes=self.args.vocab_size).float()\n",
    "\n",
    "        # Perform prediction and calculate loss and accuracy\n",
    "        preds = self.forward(inp_data, mask=None, start_pos=0)\n",
    "  \n",
    "\n",
    "        loss = F.cross_entropy(preds.view(-1,preds.size(-1)), labels.view(-1))\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "   \n",
    "        # Logging\n",
    "        self.log(f\"{mode}_loss\", loss, prog_bar=True)\n",
    "        self.log(f\"{mode}_acc\", acc, prog_bar=True)\n",
    "\n",
    "\n",
    "        return loss, acc\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "  \n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reverse(args):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"ReverseTask\")\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=10,\n",
    "                         gradient_clip_val=5)\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ReverseTask.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = ReversePredictor.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        \n",
    "        model = ReversePredictor(args)\n",
    "        # model = ReversePredictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    \n",
    "\n",
    "    model = model.to(device)\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type           | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | transformer | TransformerPos | 44.5 K | train\n",
      "1 | output_net  | Sequential     | 4.9 K  | train\n",
      "-------------------------------------------------------\n",
      "49.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "49.5 K    Total params\n",
      "0.198     Total estimated model params size (MB)\n",
      "22        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ff11fcba0f4fc0812b5e17b08116f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22432b12daf4899a36274dbbc1cb30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m args.vocab_size = train_loader.dataset.num_categories\n\u001b[32m      5\u001b[39m args.max_batch_size = \u001b[32m128\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m reverse_model, reverse_result = \u001b[43mtrain_reverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# reverse_model, reverse_result = train_reverse(input_dim=train_loader.dataset.num_categories,\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#                                               model_dim=32,\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#                                               num_heads=1,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#                                               lr=5e-4,\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#                                               warmup=50)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_reverse\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     20\u001b[39m     model = ReversePredictor(args)\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# model = ReversePredictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Test best model on validation and test set\u001b[39;00m\n\u001b[32m     25\u001b[39m val_result = trainer.test(model, val_loader, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:539\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    538\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:575\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    569\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    570\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    571\u001b[39m     ckpt_path,\n\u001b[32m    572\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    573\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    574\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:982\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    977\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m    979\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    980\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    985\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m    986\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    987\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1026\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1024\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1025\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1027\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:320\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    322\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         closure()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_ready()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_completed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:171\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    174\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1302\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(\n\u001b[32m   1272\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1273\u001b[39m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1276\u001b[39m     optimizer_closure: Optional[Callable[[], Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1277\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1278\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03m    the optimizer.\u001b[39;00m\n\u001b[32m   1280\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1300\u001b[39m \n\u001b[32m   1301\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m opt = opt_ref()\n\u001b[32m    139\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    226\u001b[39m     params_with_grad: List[Tensor] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap_closure\u001b[39m(\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     98\u001b[39m     model: \u001b[33m\"\u001b[39m\u001b[33mpl.LightningModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     99\u001b[39m     optimizer: Steppable,\n\u001b[32m    100\u001b[39m     closure: Callable[[], Any],\n\u001b[32m    101\u001b[39m ) -> Any:\n\u001b[32m    102\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    hook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    134\u001b[39m         \u001b[38;5;28mself\u001b[39m.warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[32m    309\u001b[39m \n\u001b[32m    310\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m \n\u001b[32m    316\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer.world_size > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:323\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    326\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mReversePredictor.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     loss, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_calculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mReversePredictor._calculate_loss\u001b[39m\u001b[34m(self, batch, mode)\u001b[39m\n\u001b[32m     21\u001b[39m  \u001b[38;5;66;03m# Add retain_graph=True to avoid issues\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss, acc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Isabelita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "attn_impl= \"absorb\"#Literal[\"naive\", \"absorb\"] = \"naive\" # naive -> train, absorb -> inference\n",
    "args = ModelArgs()\n",
    "args.input_dim = train_loader.dataset.num_categories\n",
    "args.vocab_size = train_loader.dataset.num_categories\n",
    "args.max_batch_size = 128\n",
    "\n",
    "reverse_model, reverse_result = train_reverse(args)\n",
    "\n",
    "# reverse_model, reverse_result = train_reverse(input_dim=train_loader.dataset.num_categories,\n",
    "#                                               model_dim=32,\n",
    "#                                               num_heads=1,\n",
    "#                                               num_classes=train_loader.dataset.num_categories,\n",
    "#                                               num_layers=1,\n",
    "#                                               dropout=0.0,\n",
    "#                                               lr=5e-4,\n",
    "#                                               warmup=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "block no position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block combining attention and feed-forward layers.\n",
    "\n",
    "    Attributes:\n",
    "        attn (nn.Module): Attention layer (MLA).\n",
    "        ffn (nn.Module): Feed-forward network (MLP or MoE).\n",
    "        attn_norm (nn.Module): Layer normalization for attention.\n",
    "        ffn_norm (nn.Module): Layer normalization for feed-forward network.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            layer_id (int): Layer index in the transformer.\n",
    "            args (ModelArgs): Model arguments containing block parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attn = MLA(args)\n",
    "        self.ffn = MLP(args.dim, args.inter_dim) #if layer_id < args.n_dense_layers else MoE(args)\n",
    "        self.attn_norm = torch.nn.RMSNorm(args.dim)\n",
    "        self.ffn_norm = torch.nn.RMSNorm(args.dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            start_pos (int): Starting position in the sequence.\n",
    "            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n",
    "            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after block computation.\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.attn_norm(x), start_pos, mask)\n",
    "        x = self.ffn_norm(x)\n",
    "        x = x + self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model with positional embeddings, multiple layers, and output projection.\n",
    "\n",
    "    Attributes:\n",
    "        max_seq_len (int): Maximum sequence length for the transformer.\n",
    "        embed (nn.Module): Embedding layer for input tokens.\n",
    "        layers (torch.nn.ModuleList): List of transformer blocks.\n",
    "        norm (nn.Module): Layer normalization applied after all blocks.\n",
    "        head (nn.Module): Output projection layer mapping to vocabulary size.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            args (ModelArgs): Model arguments containing transformer parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        self.embed = torch.nn.Linear(self.max_seq_len, args.dim)\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(args.n_layers):\n",
    "            self.layers.append(BlockPos(layer_id, args))\n",
    "        self.norm = torch.nn.RMSNorm(args.dim)\n",
    "        self.head = torch.nn.Linear(args.dim, args.vocab_size, dtype=torch.get_default_dtype())\n",
    "\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, mask: Optional[torch.Tensor], start_pos: int = 0):\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            tokens (torch.Tensor): Input tensor of token IDs with shape (batch_size, seq_len).\n",
    "            start_pos (int, optional): Starting position in the sequence for rotary embeddings. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits tensor of shape (batch_size, vocab_size).\n",
    "        \"\"\"\n",
    "        seqlen = tokens.size(1)\n",
    "        h = self.embed(tokens)\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos+seqlen]\n",
    " \n",
    "    \n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)[:, -1]\n",
    "        logits = self.head(h)\n",
    "\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "\n",
    "    def get_attention_maps(self, tokens: torch.Tensor, mask=Optional[torch.Tensor], start_pos: int = 0):\n",
    "        seqlen = tokens.size(1)\n",
    "        h = self.embed(tokens)\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos+seqlen]\n",
    "\n",
    "        attention_maps = []\n",
    "        for layer in self.layers:\n",
    "            _, attn_map = layer.attn(x=h, start_pos = start_pos, freqs_cis =freqs_cis, mask = mask, return_attention = True)\n",
    "            attention_maps.append(rearrange(attn_map, \"b s h d -> b h s d\"))\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
